{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Download wordnet for lemmatization\n",
    "- Download cmudict for syllable count\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/farhan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to /Users/farhan/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus.reader.api import *\n",
    "from nltk.corpus.reader.util import *\n",
    "nltk.download('wordnet')\n",
    "nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Variable information\n",
    "- Path of input csv files\n",
    "- Path of output csv file\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex_path = '/Users/farhan/Desktop/Baycrest Documents/Aphasia_Study/Aphasia_STM_stim_generation/Syllable_Project/SUBTLEX-US_frequency_list.csv'\n",
    "goodwords_path = '/Users/farhan/Desktop/Baycrest Documents/Aphasia_Study/Aphasia_STM_stim_generation/Syllable_Project/goodwords_cmusyllables.csv'\n",
    "output_directory = '/Users/farhan/Desktop/Baycrest Documents/Aphasia_Study/Aphasia_STM_stim_generation/Syllable_Project/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Read in the required csv files\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex = pd.read_csv(subtlex_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Function to get syllable count of word from cmudict\n",
    "- returns -1 if word not in cmudict\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict\n",
    "cdict = cmudict.dict()\n",
    "\n",
    "def syllable_count(word):\n",
    "    try:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in cdict[word.lower()]][0]\n",
    "    except KeyError:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Step 1\n",
    "- Filtering by frequency\n",
    "- Keep rows with 4 < Zipfvalue < 5\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex = subtlex.loc[subtlex.Zipfvalue < 5]\n",
    "subtlex = subtlex.loc[subtlex.Zipfvalue > 4].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Step 2a\n",
    "- Add column with syllable count\n",
    "- call syllable_count function on each word\n",
    "- use lambda function to partially vectorize process\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex[\"syllables\"] = subtlex.apply(lambda x: syllable_count(x['Word']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Step 2b\n",
    "- Add column with lemma of the words\n",
    "- Need to use lemmatizer from nltk\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an instance of lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "## Call lemmatizer on each word using lambda function\n",
    "subtlex['Word'] = subtlex.apply(lambda x: lemmatizer.lemmatize(x['Word']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Filtering by syllable count\n",
    "- Keep rows with syllables <= 2\n",
    "- Also, drop rows with unfound syllable count\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex = subtlex.loc[subtlex.syllables <= 2].reset_index(drop = True)\n",
    "\n",
    "unfound_count = len(subtlex.loc[subtlex.syllables == -1].reset_index(drop = True))\n",
    "subtlex = subtlex.loc[subtlex.syllables > 0].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Convert dataframe to SOS acceptable format\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex = subtlex.rename(columns={'Word': 'Word|s', 'FREQcount': 'FREQcount|f', 'CDcount': 'CDcount|f', 'FREQlow': 'FREQlow|f', 'Cdlow': 'Cdlow|f',\n",
    "    'SUBTLWF': 'SUBTLWF|f', 'Lg10WF': 'Lg10WF|f', 'SUBTLCD': 'SUBTLCD|f', 'Lg10CD': 'Lg10CD|f', 'Dom_PoS_SUBTLEX': 'Dom_PoS_SUBTLEX|s', \n",
    "    'Freq_dom_PoS_SUBTLEX': 'Freq_dom_PoS_SUBTLEX|f','Percentage_dom_PoS': 'Percentage_dom_PoS|f', 'All_PoS_SUBTLEX': 'All_PoS_SUBTLEX|s',\n",
    "    'All_freqs_SUBTLEX': 'All_freqs_SUBTLEX|f', 'Zipfvalue': 'Zipfvalue|f','syllables': 'syllables|f'}, errors=\"raise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Steps 1, 2, 3 done\n",
    "- Output the modified csv to run through SOS\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex.to_csv(output_directory + 'sos_input.txt', sep='\\t', index=False)\n",
    "# df.rename(columns=lambda x: x.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc6a953ebb347b6d2fdf3a98b9fbd102747a9cd82bcf39fd87dd31e2b48be30b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
