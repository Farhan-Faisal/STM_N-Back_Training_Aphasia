{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Download wordnet for lemmatization\n",
    "- Download cmudict for syllable count\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/farhan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to /Users/farhan/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/farhan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.api import *\n",
    "from nltk.corpus.reader.util import *\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "nltk.download('wordnet')\n",
    "nltk.download('cmudict')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Variable information\n",
    "- Path of input csv files\n",
    "- Path of output csv file\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex_path = '/Users/farhan/Desktop/Baycrest Documents/Aphasia_Study/Aphasia_STM_stim_generation/Syllable_Project/SUBTLEX-US_frequency_list.csv'\n",
    "profane_path = '/Users/farhan/Desktop/Baycrest Documents/Aphasia_Study/Aphasia_STM_stim_generation/Syllable_Project/profaneWords.txt'\n",
    "output_directory = '/Users/farhan/Desktop/Baycrest Documents/Aphasia_Study/Aphasia_STM_stim_generation/Syllable_Project/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Read in the required csv files\n",
    "    - Read in the list of words from SUBTLEX\n",
    "    - Read in the list of profane words to be used for cross checking\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataframe constructed from SUBTLEX\n",
    "subtlex = pd.read_csv(subtlex_path)\n",
    "\n",
    "## List of profane words\n",
    "profane = open(profane_path, 'r')\n",
    "profane_words_temp = profane.readlines()\n",
    "profane_words = []\n",
    "for words in profane_words_temp:\n",
    "    profane_words.append(words.strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Function to get syllable count of word from cmudict\n",
    "- returns -1 if word not in cmudict\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict\n",
    "cdict = cmudict.dict()\n",
    "\n",
    "def syllable_count(word):\n",
    "    try:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in cdict[word.lower()]][0]\n",
    "    except KeyError:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Step 1\n",
    "- Filtering by frequency\n",
    "- Keep rows with 4 < Zipfvalue < 5\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex = subtlex.loc[subtlex.Zipfvalue < 5]\n",
    "subtlex = subtlex.loc[subtlex.Zipfvalue > 4].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Step 2a\n",
    "- Add column with syllable count\n",
    "- call syllable_count function on each word\n",
    "- use lambda function to partially vectorize process\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex[\"syllables\"] = subtlex.apply(lambda x: syllable_count(x['Word']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Step 2b\n",
    "- Add column with lemma of the words\n",
    "- Need to use lemmatizer from nltk\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>FREQcount</th>\n",
       "      <th>CDcount</th>\n",
       "      <th>FREQlow</th>\n",
       "      <th>Cdlow</th>\n",
       "      <th>SUBTLWF</th>\n",
       "      <th>Lg10WF</th>\n",
       "      <th>SUBTLCD</th>\n",
       "      <th>Lg10CD</th>\n",
       "      <th>Dom_PoS_SUBTLEX</th>\n",
       "      <th>Freq_dom_PoS_SUBTLEX</th>\n",
       "      <th>Percentage_dom_PoS</th>\n",
       "      <th>All_PoS_SUBTLEX</th>\n",
       "      <th>All_freqs_SUBTLEX</th>\n",
       "      <th>Zipfvalue</th>\n",
       "      <th>syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aah</td>\n",
       "      <td>2688</td>\n",
       "      <td>634</td>\n",
       "      <td>52</td>\n",
       "      <td>37</td>\n",
       "      <td>52.71</td>\n",
       "      <td>3.4296</td>\n",
       "      <td>7.56</td>\n",
       "      <td>2.8028</td>\n",
       "      <td>Interjection</td>\n",
       "      <td>2657.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Interjection</td>\n",
       "      <td>2657</td>\n",
       "      <td>4.721425</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aaron</td>\n",
       "      <td>747</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.65</td>\n",
       "      <td>2.8739</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.2122</td>\n",
       "      <td>Name</td>\n",
       "      <td>744.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Name</td>\n",
       "      <td>744</td>\n",
       "      <td>4.165736</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>678</td>\n",
       "      <td>538</td>\n",
       "      <td>650</td>\n",
       "      <td>522</td>\n",
       "      <td>13.29</td>\n",
       "      <td>2.8319</td>\n",
       "      <td>6.41</td>\n",
       "      <td>2.7316</td>\n",
       "      <td>Verb</td>\n",
       "      <td>480.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>Verb.Adjective</td>\n",
       "      <td>480.200</td>\n",
       "      <td>4.123704</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abby</td>\n",
       "      <td>637</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.49</td>\n",
       "      <td>2.8048</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.0128</td>\n",
       "      <td>Name</td>\n",
       "      <td>632.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Name</td>\n",
       "      <td>632</td>\n",
       "      <td>4.096655</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ability</td>\n",
       "      <td>980</td>\n",
       "      <td>679</td>\n",
       "      <td>974</td>\n",
       "      <td>673</td>\n",
       "      <td>19.22</td>\n",
       "      <td>2.9917</td>\n",
       "      <td>8.09</td>\n",
       "      <td>2.8325</td>\n",
       "      <td>Noun</td>\n",
       "      <td>975.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Noun</td>\n",
       "      <td>975</td>\n",
       "      <td>4.283503</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3618</th>\n",
       "      <td>z</td>\n",
       "      <td>598</td>\n",
       "      <td>227</td>\n",
       "      <td>136</td>\n",
       "      <td>71</td>\n",
       "      <td>11.73</td>\n",
       "      <td>2.7774</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.3579</td>\n",
       "      <td>Letter</td>\n",
       "      <td>369.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>Letter.Name</td>\n",
       "      <td>369.43</td>\n",
       "      <td>4.069261</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3619</th>\n",
       "      <td>zack</td>\n",
       "      <td>1056</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.71</td>\n",
       "      <td>3.0241</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.0043</td>\n",
       "      <td>Name</td>\n",
       "      <td>515.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>Name.Verb.Noun</td>\n",
       "      <td>515.416.116</td>\n",
       "      <td>4.315909</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3620</th>\n",
       "      <td>zero</td>\n",
       "      <td>1094</td>\n",
       "      <td>651</td>\n",
       "      <td>694</td>\n",
       "      <td>519</td>\n",
       "      <td>21.45</td>\n",
       "      <td>3.0394</td>\n",
       "      <td>7.76</td>\n",
       "      <td>2.8142</td>\n",
       "      <td>Number</td>\n",
       "      <td>570.0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>Number.Noun.Verb.Name</td>\n",
       "      <td>570.431.83.9</td>\n",
       "      <td>4.331248</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3621</th>\n",
       "      <td>zone</td>\n",
       "      <td>1026</td>\n",
       "      <td>622</td>\n",
       "      <td>856</td>\n",
       "      <td>543</td>\n",
       "      <td>20.12</td>\n",
       "      <td>3.0116</td>\n",
       "      <td>7.42</td>\n",
       "      <td>2.7945</td>\n",
       "      <td>Noun</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Noun</td>\n",
       "      <td>1023</td>\n",
       "      <td>4.303405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>zoo</td>\n",
       "      <td>696</td>\n",
       "      <td>386</td>\n",
       "      <td>584</td>\n",
       "      <td>347</td>\n",
       "      <td>13.65</td>\n",
       "      <td>2.8432</td>\n",
       "      <td>4.60</td>\n",
       "      <td>2.5877</td>\n",
       "      <td>Noun</td>\n",
       "      <td>695.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Noun.Name</td>\n",
       "      <td>695.1</td>\n",
       "      <td>4.135067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3623 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  FREQcount  CDcount  FREQlow  Cdlow  SUBTLWF  Lg10WF  SUBTLCD  \\\n",
       "0           aah       2688      634       52     37    52.71  3.4296     7.56   \n",
       "1         aaron        747      162        0      0    14.65  2.8739     1.93   \n",
       "2     abandoned        678      538      650    522    13.29  2.8319     6.41   \n",
       "3          abby        637      102        0      0    12.49  2.8048     1.22   \n",
       "4       ability        980      679      974    673    19.22  2.9917     8.09   \n",
       "...         ...        ...      ...      ...    ...      ...     ...      ...   \n",
       "3618          z        598      227      136     71    11.73  2.7774     2.71   \n",
       "3619       zack       1056      100        0      0    20.71  3.0241     1.19   \n",
       "3620       zero       1094      651      694    519    21.45  3.0394     7.76   \n",
       "3621       zone       1026      622      856    543    20.12  3.0116     7.42   \n",
       "3622        zoo        696      386      584    347    13.65  2.8432     4.60   \n",
       "\n",
       "      Lg10CD Dom_PoS_SUBTLEX  Freq_dom_PoS_SUBTLEX  Percentage_dom_PoS  \\\n",
       "0     2.8028    Interjection                2657.0                1.00   \n",
       "1     2.2122            Name                 744.0                1.00   \n",
       "2     2.7316            Verb                 480.0                0.71   \n",
       "3     2.0128            Name                 632.0                1.00   \n",
       "4     2.8325            Noun                 975.0                1.00   \n",
       "...      ...             ...                   ...                 ...   \n",
       "3618  2.3579          Letter                 369.0                0.90   \n",
       "3619  2.0043            Name                 515.0                0.49   \n",
       "3620  2.8142          Number                 570.0                0.52   \n",
       "3621  2.7945            Noun                1023.0                1.00   \n",
       "3622  2.5877            Noun                 695.0                1.00   \n",
       "\n",
       "            All_PoS_SUBTLEX All_freqs_SUBTLEX  Zipfvalue  syllables  \n",
       "0              Interjection              2657   4.721425         -1  \n",
       "1                      Name               744   4.165736          2  \n",
       "2            Verb.Adjective           480.200   4.123704          3  \n",
       "3                      Name               632   4.096655          2  \n",
       "4                      Noun               975   4.283503          4  \n",
       "...                     ...               ...        ...        ...  \n",
       "3618            Letter.Name            369.43   4.069261          1  \n",
       "3619         Name.Verb.Noun       515.416.116   4.315909          1  \n",
       "3620  Number.Noun.Verb.Name      570.431.83.9   4.331248          2  \n",
       "3621                   Noun              1023   4.303405          1  \n",
       "3622              Noun.Name             695.1   4.135067          1  \n",
       "\n",
       "[3623 rows x 16 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create an instance of lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "## Call lemmatizer on each word using lambda function\n",
    "subtlex['Word'] = subtlex.apply(lambda x: lemmatizer.lemmatize(x['Word']), axis=1)\n",
    "subtlex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Filtering by syllable count\n",
    "- Keep rows with syllables <= 2\n",
    "- Also, drop rows with unfound syllable count\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex = subtlex.loc[subtlex.syllables <= 2].reset_index(drop = True)\n",
    "\n",
    "unfound_count = len(subtlex.loc[subtlex.syllables == -1].reset_index(drop = True))\n",
    "subtlex = subtlex.loc[subtlex.syllables > 0].reset_index(drop = True)\n",
    "# subtlex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Filter out profane words\n",
    "- Filter out Names\n",
    "- Filter out single lettered words\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop profane words\n",
    "subtlex = subtlex.loc[~subtlex.Word.isin(profane_words)].reset_index(drop = True)\n",
    "\n",
    "## Drop names\n",
    "subtlex = subtlex.loc[subtlex.Dom_PoS_SUBTLEX != 'Name'].reset_index(drop = True)\n",
    "\n",
    "## Drop single lettered words\n",
    "subtlex = subtlex.loc[subtlex.All_PoS_SUBTLEX.str.contains(\"Letter\") == False].reset_index(drop = True)\n",
    "# subtlex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Convert dataframe to SOS acceptable format\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex = subtlex.rename(columns={'Word': 'Word|s', 'FREQcount': 'FREQcount|f', 'CDcount': 'CDcount|f', 'FREQlow': 'FREQlow|f', 'Cdlow': 'Cdlow|f',\n",
    "    'SUBTLWF': 'SUBTLWF|f', 'Lg10WF': 'Lg10WF|f', 'SUBTLCD': 'SUBTLCD|f', 'Lg10CD': 'Lg10CD|f', 'Dom_PoS_SUBTLEX': 'Dom_PoS_SUBTLEX|s', \n",
    "    'Freq_dom_PoS_SUBTLEX': 'Freq_dom_PoS_SUBTLEX|f','Percentage_dom_PoS': 'Percentage_dom_PoS|f', 'All_PoS_SUBTLEX': 'All_PoS_SUBTLEX|s',\n",
    "    'All_freqs_SUBTLEX': 'All_freqs_SUBTLEX|f', 'Zipfvalue': 'Zipfvalue|f','syllables': 'syllables|f'}, errors=\"raise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Steps 1, 2, 3 done\n",
    "- Output the modified csv to run through SOS\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex.to_csv(output_directory + 'sos_input.txt', sep='\\t', index=False)\n",
    "# df.rename(columns=lambda x: x.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
