{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Download wordnet for lemmatization\n",
    "- Download cmudict for syllable count\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "# nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Variable information\n",
    "- Path of input csv files\n",
    "- Path of output csv file\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex_path = '/Users/farhan/Desktop/Baycrest Documents/Aphasia_Study/Aphasia_STM_stim_generation/Syllable_Project/SUBTLEX-US_frequency_list.csv'\n",
    "goodwords_path = '/Users/farhan/Desktop/Baycrest Documents/Aphasia_Study/Aphasia_STM_stim_generation/Syllable_Project/goodwords_cmusyllables.csv'\n",
    "output_directory = '/Users/farhan/Desktop/Baycrest Documents/Aphasia_Study/Aphasia_STM_stim_generation/Syllable_Project/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Read in the required csv files\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex = pd.read_csv(subtlex_path)\n",
    "goodwords = pd.read_csv(goodwords_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Function to get syllable count of word from cmudict\n",
    "- returns -1 if word not in cmudict\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdict = cmudict.dict()\n",
    "\n",
    "def syllable_count(word):\n",
    "    try:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in cdict[word.lower()]][0]\n",
    "    except KeyError:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Step 1\n",
    "- Filtering by frequency\n",
    "- Keep rows with 4 < Zipfvalue < 5\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex = subtlex.loc[subtlex.Zipfvalue < 5]\n",
    "subtlex = subtlex.loc[subtlex.Zipfvalue > 4].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Step 2a\n",
    "- Add column with syllable count\n",
    "- call syllable_count function on each word\n",
    "- use lambda function to partially vectorize process\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex[\"syllables\"] = subtlex.apply(lambda x: syllable_count(x['Word']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Step 2b\n",
    "- Add column with lemma of the words\n",
    "- Need to use lemmatizer from nltk\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an instance of lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "## Call lemmatizer on each word using lambda function\n",
    "subtlex['lemma'] = subtlex.apply(lambda x: lemmatizer.lemmatize(x['Word']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Filtering by syllable count\n",
    "- Keep rows with syllables <= 2\n",
    "- Also, drop rows with unfound syllable count\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtlex = subtlex.loc[subtlex.syllables <= 2].reset_index(drop = True)\n",
    "\n",
    "unfound_count = len(subtlex.loc[subtlex.syllables == -1].reset_index(drop = True))\n",
    "subtlex = subtlex.loc[subtlex.syllables > 0].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Steps 1, 2, 3 done\n",
    "- Output the modified csv to run through SOS\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'Word',\n",
       " 'FREQcount',\n",
       " 'CDcount',\n",
       " 'FREQlow',\n",
       " 'Cdlow',\n",
       " 'SUBTLWF',\n",
       " 'Lg10WF',\n",
       " 'SUBTLCD',\n",
       " 'Lg10CD',\n",
       " 'Dom_PoS_SUBTLEX',\n",
       " 'Freq_dom_PoS_SUBTLEX',\n",
       " 'Percentage_dom_PoS',\n",
       " 'All_PoS_SUBTLEX',\n",
       " 'All_freqs_SUBTLEX',\n",
       " 'Zipfvalue',\n",
       " 'syllables',\n",
       " 'lemma']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtlex.to_csv(output_directory + 'sos_input.csv') \n",
    "kikos = pd.read_csv(output_directory + 'sos_input.csv')\n",
    "kikos.columns.to_list()\n",
    "# df.rename(columns=lambda x: x.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc6a953ebb347b6d2fdf3a98b9fbd102747a9cd82bcf39fd87dd31e2b48be30b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
