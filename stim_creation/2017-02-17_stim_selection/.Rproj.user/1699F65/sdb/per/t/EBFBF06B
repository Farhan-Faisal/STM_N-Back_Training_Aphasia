{
    "collab_server" : "",
    "contents" : "################################################################################\n# Author: Tiffany Deschamps\n# Date: February 10, 2017\n# Last updated by Tiffany Deschamps on January 20, 2017\n################################################################################\n\n################################################################################\n# THE PLAN:\n# 1. Extract nouns from SUBTLEX-US corpus.\n# 2. Remove words included in Regina's treatment list.\n# 3. Perform lemmatization & count syllables in python.\n# 4. Extract one-syllable words and three-syllable words from this new list.\n# 5. Enter nouns into N-WATCH to get bigram frequency and length.\n# 6. Use SOS to sample from this population of nouns to create stimulus lists.\n################################################################################\n\n################################################################################\n# STEP 1: EXTRACT NOUNS FROM SUBTLEX-US CORPUS\n################################################################################\n\nlibrary(openxlsx) # load library with commands to open .xlsx files\n\n# load SUBTLEX-US corpus\nlist.files(\"./data\")\nsubtlexus <- read.xlsx(\"./data/SUBTLEX-US frequency list with PoS and Zipf information.xlsx\",\n                       sheet = 1)\n\n# extract nouns from subtlexus\nnouns <- subset(subtlexus, Dom_PoS_SUBTLEX == \"Noun\")\nwrite.csv(nouns, file = \"nouns.csv\")\n\n################################################################################\n# STEP 2: REMOVE WORDS ALSO IN TREATMENT LIST\n################################################################################\n\n# load treatment words\ntreatment <- read.xlsx(\"./data/2017-02-09_baseline_measures_294.xlsx\", sheet = 1)\n\nfor (i in 1:length(treatment$Target)) {\n      # print progress\n      print(cat(paste0(\"Processing item \", i, \" of 285, \",\n                       round((i / length(treatment) * 100), 3),\n                       \"% completed\")))\n      # look for treatment word in nouns list, remove if present\n      for (j in 1:length(nouns$Word)) {\n            if (treatment$Target[i] == nouns$Word[j]) {\n                  nouns <- nouns[-i, ]\n                  break\n            }\n      }\n}\n\n################################################################################\n# STEP 3: LEMMATIZE NOUNS & GET SYLLABLE INFO IN PYTHON\n################################################################################\n\n# take only the nouns that are between 3 and 10 characters long\ngoodnouns <- subset(nouns, nchar(Word) >= 3 & nchar(Word) <= 10)\n\n# write goodnouns to .csv to use in python script\nwrite.csv(goodnouns, file = \"./data/goodnouns.csv\")\n\n# the rest of this step happens in python\n\n################################################################################\n# STEP 4: EXTRACT 1- AND 3- SYLLABLE WORDS\n################################################################################\n\nnewnouns <- read.csv(\"./data/word_rep_pop_sylls.csv\", header = TRUE)\n\nrightnouns <- subset(newnouns, CMUSylls == 1 | CMUSylls == 3)\n\n# write rightnouns to be input into N-WATCH to obtain orthographic information\nwrite.csv(rightnouns, file=\"population_nouns.csv\")\n\n################################################################################\n# STEP 5: RUN WORDS THROUGH N-WATCH TO GET ORTHOGRAPHIC INFO\n################################################################################\n\n# the first part of this step happens in N-WATCH\n# select bigram freq (type), length, and neighbourhood count\n\nnwatch <- nwatchvars <- read.table(\"popnouns_nwatch.txt\", header = TRUE)\n      \npopulation <- cbind(rightnouns, nwatch[, 2:4])\n\nwrite.csv(population, file = \"2017-02-13_population_allvars.csv\")\n\n\n\n##### FOR SOME REASON, I DIDN'T TAKE THE TREATMENT WORDS OUT OF THE\n##### LIST OF WORDS WE'RE USING, SO I'M DOING IT NOW\n\nnewpop <- read.xlsx(\"2017-02-13_updated_population_allvars_lemmatized_TDedit_PSBedit.xlsx\")\nprint(newpop$Word[2381])\n\n# load treatment words\n\nfor (i in 1:length(treatment$Target)) {\n      # print progress\n      print(cat(paste0(\"Processing item \", i, \" of 293, \",\n                       round((i / length(treatment$Target) * 100), 2),\n                       \"% completed.\")))\n      # look for treatment word in nouns list, remove if present\n      for (j in 1:(length(newpop$Word)-1)) {\n            if (treatment$Target[i] == newpop$Word[j]) {\n                  newpop <- newpop[-j, ]\n            }\n      }\n}\nwrite.csv(newpop, \"2017-02-17_updated_population_allvars_lemmatized_tx-removed.csv\")\n",
    "created" : 1486748554138.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4057318174",
    "id" : "EBFBF06B",
    "lastKnownWriteTime" : 1487366216,
    "last_content_update" : 1487366216829,
    "path" : "~/NIBS_treatment/word_rep_paradigm/stimuli_creation/2017-02-09_stim_selection/2017-02-09_stim_selection.R",
    "project_path" : "2017-02-09_stim_selection.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}