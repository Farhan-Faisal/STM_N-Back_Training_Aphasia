{
    "collab_server" : "",
    "contents" : "################################################################################\n#  STIMULI SELECTION FOR WORD REPETITION EXPERIMENT FOR NIBS TREATMENT STUDY   #\n#  2016-12-14                                                                  #\n################################################################################\n\n################################################################################\n# THE PLAN:\n# 1. Extract nouns and verbs from SUBTLEX-US corpus.\n# 2. Enter nouns and verbs into N-WATCH to retrieve additional linguistic \n#    features (plan to extract all features N-WATCH offers).\n# 3. Combine frequency norms from SUBTLEX-US and linguistic info from N-WATCH.\n# 4. Extract one-syllable words and three-syllable words from this new database.\n# 5. Divide the data to create lists of words that roughly fit into these \n#    conditons:\n#    a. high-frequency one-syllable words\n#    b. low-frequency one-syllable words\n#    c. high-frequency three-syllable words\n#    d. low-frequency three-syllable words\n# 6. Sample data to create a stimulus list of 400 words, 100 in each condition,\n#    with two lists of 50 words in each condition, all matched on various \n#    linguistic factors (AoA, # of phonemes, concreteness, etc).\n# 7. Maybe we can come up with 2000 words, 500 in each condition, with 10 lists\n#    of 50 words in each condition, all matched on various linguistic factors?\n################################################################################\n\n\n################################################################################\n# STEP 1 - EXTRACT NOUNS AND VERBS FROM SUBTLEX-US CORPUS                      #\n################################################################################\n\n# Load the SUBTLEX-US corpus, which contains word frequency data collected from\n# the subtitles of American films and television shows.\nlibrary(openxlsx)\nlist.files(\"./data\")\nsubtlexus <- read.xlsx(\"./data/SUBTLEX-US frequency list with PoS and Zipf information.xlsx\",\n                       sheet = 1)\n\n# Extract all nouns that are only used as nouns from the SUBTLEX-US corpus.\nnouns <- subtlexus[ which(subtlexus$Dom_PoS_SUBTLEX == \"Noun\" & \n                                subtlexus$All_PoS_SUBTLEX == \"Noun\"), ]\n\n# Extract all verbs that are only used as verbs from the SUBTLEX-US corpus.\nverbs <- subtlexus[which(subtlexus$Dom_PoS_SUBTLEX == \"Verb\" &\n                               subtlexus$All_PoS_SUBTLEX == \"Verb\"), ]\n\n# Combine nouns and verbs into a single data frame and write to .xlsx\nnounsandverbs <- rbind(nouns, verbs)\nwrite.csv(nounsandverbs, file = \"subtlexus_nounsandverbs.csv\")\n\n################################################################################\n# STEP 2 - ENTER NOUNS AND VERBS INTO N-WATCH, RETRIEVE LING INFO              #\n################################################################################\n\n# This step will be completed outside of R, in the N-WATCH program.\n\n#####\n# NOTE: N-WATCH doesn't like words that are less than 2 letters long, or more\n# than 12 letters long. It also doesn't calculate bigram frequencies for words\n# more than 10 letters long. So, I'm subsetting the data to only include\n# words between 3 and 10 letters long.\n#####\n\ngoodnounsandverbs <- subset(nounsandverbs, nchar(Word) >= 3 & nchar(Word) <= 10)\nwrite.csv(goodnounsandverbs, file = \"subtlexus_nounsandverbs_3-10chars.csv\")\n\n#####\n# NOTE: N-WATCH has trouble with computing ALL of the linguistic features for\n# all ~30,000 words, so the first one I extracted was the number of syllables.\n# Using the number of syllables, I can limit the list of words I give N-WATCH\n# to only words that fit our syllable-length criteria (1 vs. 3). Hopefully\n# N-WATCH can handle that new list?\n#####\n\n# load the syllable info\nlist.files(\"./\")\nnumsylls <- read.delim(\"./n-watch_numsylls.txt\")\n\n# append the syllable info to the 'goodnounsandverbs' data frame\nnumsylls <- numsylls[1:29959, ] # remove last empty row\ngoodnounsandverbs <- cbind(goodnounsandverbs, syllables = numsylls$LEN_S)\ngoodnounsandverbs$syllables <- as.character(goodnounsandverbs$syllables)\ngoodnounsandverbs$syllables <- as.numeric(goodnounsandverbs$syllables)\n\n\n# this code loses A LOT of data\n# # subset only 1- and 3-syllable words\n# rightnounsandverbs <- subset(goodnounsandverbs, syllables == 1 | syllables == 3)\n# write.csv(rightnounsandverbs, \n#           file = \"subtlexus_nounsandverbs_3-10chars_1-3sylls.csv\")\n\n\n#####\n# NOTE: N-WATCH only gave us information on number of syllables for 12874 of\n# the 29959 words that were 3-10 letters long, so we're going to use the CMU\n# Dictionary to estimate the number of syllables.\n#####\n\n# read in the CMU dictionary\ncmudict <- read.delim(\"./cmudict-edited.txt\", header = FALSE)\n\n# this separates the word from the pronunciation\nlibrary(tidyr)\ncmudictsep <- cmudict %>% separate(V1, into = c(\"word\", \"pronunciation\"), \n                                   sep = \" \", extra = \"merge\", fill = \"right\")\nwrite.csv(cmudictsep, file = \"cmudict.csv\")\n\n# now, we have to find a way to count the number of syllables\n# to do this, we're going to count the number of digits in the character\n# string in the pronunciation column, then input that digit into a new column\n# called \"syllables\"\n\ncmudict <- cmudictsep # moves separated data to old name\ncmudict[, \"syllables\"] <- NA # add a syllables column\n\n# this counts the numbers of digits in the pronunciation and saves the result\n# in the syllable column\nfor (i in 1:length(cmudict$pronunciation)) {\n      # count number of digits in CMU pronunciation (digits = syllables)\n      syllnum <- as.numeric(length(unlist(regmatches(cmudict$pronunciation[i], \n                                                     gregexpr(\"[0-9]\", \n                                                              cmudict$pronunciation[i])))))\n      # write number of syllables into cmudict\n      cmudict$syllables[i] <- syllnum\n}\n\n# translate upper case to lower case to make matching easier\ncmudict$word <- tolower(cmudict$word)\nwrite.csv(cmudict, file = \"cmudict_syllables.csv\") # write file to csv\n\n\n# lookup words from goodnounsandverbs in cmudict, extract number of syllables,\n# then write number of syllables into goodnounsandverbs\ngoodnounsandverbs[, \"cmu_syllss\"] <- NA\n\n# define the vector of letter indices in the dict\n# indexvec <- data.frame(word = c(\"a\", \"b\", \"c\"), index = c(1, 2, 3))\nletters <- c(\"a\",\"b\",\"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \n             \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\")\nindices <- c(1, 7236, 16918, 27611, 35349, 40052, 45296, 51003, 57445, 60833,\n             62502, 66658, 72166, 81695, 84900, 87882, 96128, 96584, 103912, \n             117905, 123540, 125340, 127670, 132056, 132135, 132863, 13379)\n\nfor (i in 1:length(goodnounsandverbs$Word)) {\n      # print(i)\n      print(cat(paste0(\"Processing item \", i, \" of 29959, \", \n                       round((i / length(goodnounsandverbs$Word) * 100), 3), \n                       \"% completed\")))\n      # look at first letter\n      fletter <- substring(goodnounsandverbs$Word[i], 1, 1)\n      \n      # find position in the alphabet of first letter\n      indx <- match(fletter,letters)\n      \n      # this is going to be the index at which to start\n      jstart <- indices[indx]\n      jstop <- indices[indx+1]\n      \n      # loop over dict\n      for (j in jstart:jstop) {\n            if (goodnounsandverbs$Word[i] == cmudict$word[j]) {\n                  syllnum <- cmudict$syllables[j]\n                  goodnounsandverbs$cmu_sylls[i] <- syllnum\n                  break\n            } \n      }\n}\nwrite.csv(goodnounsandverbs, file = \"goodwords_cmusyllables.csv\") # write file\n\n################################################################################\n# STEP 3 - COMBINE FREQUENCY NORMS WITH OTHER LING INFO FROM N-WATCH           #\n################################################################################\n",
    "created" : 1481906315041.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "439170834",
    "id" : "20F54173",
    "lastKnownWriteTime" : 1483456356,
    "last_content_update" : 1483456356134,
    "path" : "~/NIBS_treatment/word_rep_paradigm/stimuli_creation/2016-12-14_stimuli_selection/2017-01-03_stimuli_selection.R",
    "project_path" : null,
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}